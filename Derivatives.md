# ðŸ§® Common Derivatives Cheatsheet for AI Research Scientist Interviews

This sheet covers the derivatives you should **memorize cold** â€” especially for PyTorch autograd and manual gradient derivations.

---

## ðŸ”¹ Fundamental Derivatives

| Function | Derivative |
|-----------|-------------|
| d/dx (x^n) | nÂ·x^(nâˆ’1) |
| d/dx (e^x) | e^x |
| d/dx (a^x) | a^xÂ·ln(a) |
| d/dx (ln(x)) | 1/x |
| d/dx (log_a(x)) | 1 / (xÂ·ln(a)) |

---

## ðŸ”¹ Trigonometric

| Function | Derivative |
|-----------|-------------|
| sin(x) | cos(x) |
| cos(x) | âˆ’sin(x) |
| tan(x) | secÂ²(x) |
| sec(x) | sec(x)Â·tan(x) |
| csc(x) | âˆ’csc(x)Â·cot(x) |
| cot(x) | âˆ’cscÂ²(x) |

---

## ðŸ”¹ Inverse Trigonometric

| Function | Derivative |
|-----------|-------------|
| sinâ»Â¹(x) | 1 / âˆš(1 âˆ’ xÂ²) |
| cosâ»Â¹(x) | âˆ’1 / âˆš(1 âˆ’ xÂ²) |
| tanâ»Â¹(x) | 1 / (1 + xÂ²) |

---

## ðŸ”¹ Exponential and Logarithmic Combinations

| Function | Derivative |
|-----------|-------------|
| e^(ax) | aÂ·e^(ax) |
| ln(ax) | 1/x |
| x^x | x^x (1 + ln(x)) |

---

## ðŸ”¹ Neural Network Activations

| Function | Derivative |
|-----------|-------------|
| Sigmoid: Ïƒ(x) = 1 / (1 + e^(âˆ’x)) | Ïƒ(x)(1 âˆ’ Ïƒ(x)) |
| tanh(x) | 1 âˆ’ tanhÂ²(x) |
| ReLU(x) | 1 if x>0 else 0 |
| LeakyReLU(x) | 1 if x>0 else Î± |
| Softplus(x) = ln(1 + e^x) | 1 / (1 + e^(âˆ’x)) = Sigmoid(x) |

---

## ðŸ”¹ PyTorch & Transformer-Relevant Gradients

| Function | Derivative |
|-----------|-------------|
| Softmax(xáµ¢) = e^(xáµ¢)/Î£â±¼ e^(xâ±¼) | âˆ‚yáµ¢/âˆ‚xâ±¼ = yáµ¢(Î´áµ¢â±¼ âˆ’ yâ±¼) |
| Cross Entropy: âˆ’Î£ yÂ·log(Å·) | âˆ‚L/âˆ‚logits = Å· âˆ’ y |
| Mean(x) | 1/n |
| Variance(x) | 2(x âˆ’ Î¼)/n |
| ||x||Â² = xáµ€x | 2x |
| BatchNorm Î¼ = mean(x), ÏƒÂ² = var(x) | âˆ‚y/âˆ‚x includes normalization and affine params |

---

## ðŸ”¹ Core Calculus Rules

| Rule | Formula |
|------|----------|
| Chain rule | (f(g(x)))â€² = fâ€²(g(x))Â·gâ€²(x) |
| Product rule | (uv)â€² = uâ€²v + uvâ€² |
| Quotient rule | (u/v)â€² = (uâ€²v âˆ’ uvâ€²) / vÂ² |

---

âœ… **Tip for PyTorch interviews**: be able to derive these manually and verify with `torch.autograd.gradcheck`.