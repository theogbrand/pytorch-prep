# Tokenization
- Write BPE algorithm from scratch in Python

# BackProp:
- Backprop by hand Cross-Entropy Loss for a simple 2-layer MLP with 1 hidden layer
    - BatchNorm backward pass by hand

# PyTorch:
- Implement Softmax, MLP Forward Pass, Attention Block, Batch/Layer Norm, Custom ReLU using torch.nn.Functional, then without
    - Instead of loss.backward(), manually implement backprop for particular operation and show it adds up.
- Botched implementation of above, fix it.

# Challenging (Medium to hard difficulty):
- Implement LoRA in a Linear Layer (TensorGym) - Hard
- Int8 Quantized MatMul in PyTorch (LeetGPU) - Hard