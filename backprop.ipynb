{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# ReLU(x) = max(0, x)\n",
    "def forward(ctx,x):\n",
    "    ctx.save_for_backward(x)\n",
    "    # return torch.max(x, torch.tensor(0.0)) # scalar, broadcasts 0.0 to perform elem wise comparison\n",
    "    return torch.max(x, torch.zeros_like(x)) # auto-matches device + dtype, avoids unnecessary BT\n",
    "\n",
    "def backward(ctx, grad_output):\n",
    "    input, = ctx.saved_tensors\n",
    "    grad_output = grad_output.clone()\n",
    "    return grad_output * (input > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d252c",
   "metadata": {},
   "source": [
    "# Chain Rule Conceptually:\n",
    "- when loss.backward() is called, grad_output is received. \n",
    "- So, grad_output = dLoss/d(ReLU output)\n",
    "- grad_ReLU_input (what we want to calculate, since this becomes the grad_output for the previous layer and \"back propagates\") \n",
    "    - grad_ReLU_input = dLoss/d(ReLU input) -> use this as LHS reference point\n",
    "\n",
    "- By chain rule, dLoss/d(input) = dLoss/d(ReLU output) * d(ReLU output)/d(input)\n",
    "    - we know dLoss/d(ReLU output) from grad_output, we need to calculate d(ReLU output)/d(input)\n",
    "    - d(ReLU output)/d(input) = 0 if input < 0, 1 if input > 0\n",
    "        - or derive it analytically using calculus (or handled by autodifferentiation). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fca49f",
   "metadata": {},
   "source": [
    "- The local derivative is specifically the gradient of one operation (one edge) with respect to its direct inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacf1c4",
   "metadata": {},
   "source": [
    "    [x]  ← input node\n",
    "      |\n",
    "    [ReLU] ← operation (edge)\n",
    "      |\n",
    "     [y]  ← output node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c1226",
   "metadata": {},
   "source": [
    "[input] → [Linear] → [a] → [ReLU] → [b] → [Linear] → [Loss]\n",
    "\n",
    "## BackProp Sequential Process\n",
    "     \n",
    "- The point is to get the gradients at each Tensor ([a], [b], [input]). Loss is a scalar (special case), so its gradient is always 1.\n",
    "- We care about dLoss/db, dLoss/da, dLoss/dinput since they are the gradients of the corresponding tensors;\n",
    "   - we use the local gradients of each operation (Linear, ReLU, Linear) to calculate them\n",
    "      - Local Gradient 3 = dLinear/db\n",
    "      - Local Gradient 2 = dReLU/da\n",
    "      - Local Gradient 1 = dLinear/dinput\n",
    "- The \"gradients\" that always pass through are dLoss/d(input of each operation) which becomes the dLoss/d(output of previous operation) for the previous layer.\n",
    "\n",
    "When `loss.backward()` is called, backprop works **layer by layer, right to left**, and the multiplication happens **incrementally**, not all at once. Remember: always pass a derivative with \"dLoss\" as numerator to the previous layer. \n",
    "\n",
    "### Step-by-Step:\n",
    "\n",
    "1. **Start**: `∂Loss/∂Loss = 1` (implicit) always.\n",
    "\n",
    "2. **Last Linear backward** (Right to Left): \n",
    "   - Receives: `grad_output = 1` (dLoss/d(Loss)) \n",
    "   - Computes: `local grad 3 = ∂(Loss)/∂b`\n",
    "   - Returns: dLoss/db; By chain rule, dLoss/db = dLoss/d(Linear) * d(Linear)/db `grad_input = 1 × local grad 3 = ∂Loss/∂b`\n",
    "\n",
    "3. **ReLU backward** (your custom function):\n",
    "   - Receives: `grad_output = ∂Loss/∂b` (db/dReLu)\n",
    "   - Computes: `local grad 2 = ∂b/∂a` (dReLu/da)\n",
    "   - Returns: dLoss/da = dLoss/db * db/da `grad_input = (∂Loss/∂b) × (∂b/∂a) = ∂Loss/∂a`\n",
    "\n",
    "4. **First Linear backward**:\n",
    "   - Receives: `grad_output = ∂Loss/∂a`\n",
    "   - Computes: `local grad 1 = ∂a/∂input`\n",
    "   - Returns: `grad_input = (∂Loss/∂a) × (∂a/∂input) = ∂Loss/∂input`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005ba39",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
