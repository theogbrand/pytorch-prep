{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## makemore: becoming a backprop ninja\n",
    "\n",
    "swole doge style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there no change change in the first several cells from last lecture\n",
    "# !uv pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "# NOT n_dim yet!\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    # Sliding Window to create training examples\n",
    "    context = [0] * block_size # starts with all dots since itos[0] is [., ., .]\n",
    "    for ch in w + '.': # \"yuhang\" + \".\" = \"yuhang.\" where . is EOS\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix) # first char of training word\n",
    "      context = context[1:] + [ix] # Slide 1 position and append first char to the end\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'... -> y'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([itos[i.item()] for i in Xtr[0]]) + ' -> ' + itos[Ytr[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([itos[i.item()] for i in Xtr[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3339, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n * hprebn.sum(dim=0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2 # for var calculation, which is (x-m)**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(dim=0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv # final BN parameters\n",
    "hpreact = bngain * bnraw + bnbias # Y * batch-normed-activations + bias; Y & bias are learnable parameters\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer # recall BT \"pads\" dim of 1 on the LHS!! (batch dim)\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(dim=1, keepdim=True).values # returns index and values, hence .values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability to make sure exp() doesn't overflow; but actually doesn't change the probs and the loss numerically as a normalization function - conceptually gradient should be zero\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv # 2 ops happen here, first broadcast counts_sum_inv [32,1] to [32,32] then elem-wise multiply\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [6]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1,1,1], [2,2,2]])\n",
    "t.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9876, -3.2679, -4.2557,  ..., -3.5156, -2.6490, -4.1408],\n",
       "        [-3.8504, -3.0104, -3.2727,  ..., -3.6644, -2.9630, -3.9250],\n",
       "        [-2.7379, -3.6762, -3.5659,  ..., -3.5298, -3.8706, -3.5566],\n",
       "        ...,\n",
       "        [-2.6108, -3.2625, -2.7118,  ..., -3.3846, -3.8516, -3.2747],\n",
       "        [-3.6576, -4.3562, -3.3497,  ..., -3.9204, -3.3097, -3.0140],\n",
       "        [-4.1208, -3.0477, -3.2311,  ..., -2.8811, -3.0987, -3.0956]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[:batch_size, Yb] # be very careful** :batch_size is a slice of all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.9876, -3.0104, -3.5659, -3.2171, -4.0459, -3.4697, -3.1086, -3.9004,\n",
       "        -3.2368, -4.3097, -3.2192, -1.6306, -2.7989, -2.9557, -2.9373, -3.2396,\n",
       "        -3.8336, -2.9680, -3.6790, -3.3856, -2.9150, -2.9980, -4.3487, -4.0800,\n",
       "        -3.4743, -2.8488, -2.9697, -3.9927, -2.7677, -3.3846, -3.3097, -3.0956],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n), Yb] # range(n) is array [0,1...32]; Yb is array [label0, label1,...label31]; same shape so pair them elem wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((32,27))\n",
    "torch.zeros([32,27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c = a * b\n",
    "# dc/da = b\n",
    "# dprobs/dcounts_sum_inv = counts\n",
    "counts_sum_inv.shape, counts.shape, counts_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how gradient in \"SUM\" operations passes through\n",
    "# addition's gradient \"passes through\"\n",
    "# a11 a12 a13 -> b1 (a11 + a12 + a13)\n",
    "# db1/da11/12/13 is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how gradient in \"max normalization\" operation passes through\n",
    "# c is the output norm_logits\n",
    "# c11 c12 c13 = a11 a12 a13 - b1 (the max for dim=0, which is BT for elem-wise op)\n",
    "\n",
    "# dc11 = a11 - b11\n",
    "# dc/da = 1, dc/db = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_maxes = logits.max(dim=1, keepdim=True).values # returns index and values, hence .values\n",
    "# scatter dlogit_maxes into the correct index to where the maximum value is\n",
    "# [32, 27] -> each row we find the maximum value -> [32,1]\n",
    "# at each position the grad flowing through is (1 * grad_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11a54a5d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, logit_maxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer matmul gradient passing\n",
    "![multiplies and adds](2025-10-16-21-39-57.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64, 27]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits.shape, h.shape, W2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \"Local Gradient\"\n",
    "# Core Concept: the RHS terms contain the grad_out, pass it to each term on the LHS \n",
    "dlogprobs = torch.zeros_like(logprobs) # loss = -(a+b+c)/3; dloss/da = -1/3\n",
    "dlogprobs[range(n), Yb] = -(1/n)\n",
    "\n",
    "dprobs = (1/probs) * dlogprobs # dlog/dprobs * grad_out (dlogprobs)\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True) # sum to ensure shape is consistent with counts_sum_inv**\n",
    "dcounts = (counts_sum_inv * dprobs) # \"counts\" exists in another branch, we need to add later\n",
    "\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "dnorm_logits = counts * dcounts\n",
    "\n",
    "dlogits = dnorm_logits.clone() # for safety, continue later\n",
    "dlogit_maxes = -dnorm_logits.sum(dim=1, keepdim=True)\n",
    "\n",
    "# dlogits2 = torch.zeros_like(logits)\n",
    "# dlogits2[range(n), logits.max(dim=1, keepdim=True).indices] = 1 # like we did for mean(), setting certain column vals to 1\n",
    "# dlogits += dlogits2 * dlogit_maxes\n",
    "\n",
    "# alternatively use\n",
    "dlogits += F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1]) * dlogit_maxes # BT but doesn't matter since one-hot anyway\n",
    "\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits # make the shapes work out\n",
    "db2 = dlogits.sum(0)\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "# cmp('hpreact', dhpreact, hpreact)\n",
    "# cmp('bngain', dbngain, bngain)\n",
    "# cmp('bnbias', dbnbias, bnbias)\n",
    "# cmp('bnraw', dbnraw, bnraw)\n",
    "# cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "# cmp('bnvar', dbnvar, bnvar)\n",
    "# cmp('bndiff2', dbndiff2, bndiff2)\n",
    "# cmp('bndiff', dbndiff, bndiff)\n",
    "# cmp('bnmeani', dbnmeani, bnmeani)\n",
    "# cmp('hprebn', dhprebn, hprebn)\n",
    "# cmp('embcat', dembcat, embcat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "# cmp('emb', demb, emb)\n",
    "# cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \"Local Gradient\" \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]):\n",
    "    ix = Xb[k,j]\n",
    "    dC[ix] += demb[k,j]\n",
    "    \n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.337064743041992 diff: -4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0695, 0.0834, 0.0187, 0.0546, 0.0193, 0.0819, 0.0280, 0.0379, 0.0171,\n",
       "        0.0343, 0.0344, 0.0376, 0.0376, 0.0283, 0.0327, 0.0141, 0.0089, 0.0200,\n",
       "        0.0158, 0.0563, 0.0500, 0.0210, 0.0239, 0.0676, 0.0608, 0.0241, 0.0220],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0695,  0.0834,  0.0187,  0.0546,  0.0193,  0.0819,  0.0280,  0.0379,\n",
       "        -0.9829,  0.0343,  0.0344,  0.0376,  0.0376,  0.0283,  0.0327,  0.0141,\n",
       "         0.0089,  0.0200,  0.0158,  0.0563,  0.0500,  0.0210,  0.0239,  0.0676,\n",
       "         0.0608,  0.0241,  0.0220], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10866e8d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7730\n",
      "  10000/ 200000: 2.1811\n",
      "  20000/ 200000: 2.4165\n",
      "  30000/ 200000: 2.4416\n",
      "  40000/ 200000: 1.9832\n",
      "  50000/ 200000: 2.3010\n",
      "  60000/ 200000: 2.3774\n",
      "  70000/ 200000: 2.0541\n",
      "  80000/ 200000: 2.3392\n",
      "  90000/ 200000: 2.2653\n",
      " 100000/ 200000: 1.9039\n",
      " 110000/ 200000: 2.3103\n",
      " 120000/ 200000: 1.9835\n",
      " 130000/ 200000: 2.4489\n",
      " 140000/ 200000: 2.2817\n",
      " 150000/ 200000: 2.1748\n",
      " 160000/ 200000: 1.9254\n",
      " 170000/ 200000: 1.8304\n",
      " 180000/ 200000: 2.0554\n",
      " 190000/ 200000: 1.8966\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "  #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0701780319213867\n",
      "val 2.108935594558716\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah.\n",
      "amelle.\n",
      "khyimleige.\n",
      "tyrencessa.\n",
      "jazonel.\n",
      "delynn.\n",
      "jareei.\n",
      "nellara.\n",
      "chaiivan.\n",
      "leigh.\n",
      "ham.\n",
      "joce.\n",
      "quinn.\n",
      "salin.\n",
      "alianni.\n",
      "watell.\n",
      "dearisika.\n",
      "jenni.\n",
      "sabeed.\n",
      "edi.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
