- *Computational Graphs*: Nodes are operations, edges are tensors. Used for automatic differentiation (AD). Recall the Kaparthy diagram for simple differentiation. Show how gradients "flow through" from the output to the input (Left to Right), depending on the operation (+, *, etc.). and chain rule.
- *Dot Product*: The sum of the element-wise product of two vectors.
- *Matrix Multiplication*: Apply dot product to each row of the first matrix with each column of the second matrix.
- *Feed Forward Network*: The "non-linearity" part of the transformer block, where the "information processing" happens. Usually a standard 2-layer MLP. Attention routes information, FFN processes it.
- *Embedding Size*
- *Block Size*
- *N_Attention_Heads*
- *Token Embeddings (wte)*
- *Positional Embeddings (wpe)*
- *Queries (Q)*: What current word is looking for
- *Keys (K)*: Relevance of other words in the context ("how much to attend to this word")
- *Values (V)*: Actual information of the words in the context