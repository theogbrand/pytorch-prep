- *Computational Graphs*: Nodes are operations, edges are tensors. Used for automatic differentiation (AD). Recall the Kaparthy diagram for simple differentiation. Show how gradients "flow through" from the output to the input (Left to Right), depending on the operation (+, *, etc.). and chain rule.
- *Dot Product*: The sum of the element-wise product of two vectors.
- *Matrix Multiplication*: Apply dot product to each row of the first matrix with each column of the second matrix.
- *Feed Forward Network*: The "non-linearity" part of the transformer block, where the "information processing" happens. Usually a standard 2-layer MLP. Attention routes information, FFN processes it.
- *Embedding Size*
- *Block Size*
- *N_Attention_Heads*
- *Token Embeddings (wte)*
- *Positional Embeddings (wpe)*
- *Queries (Q)*: What current word is looking for
- *Keys (K)*: Relevance of other words in the context ("how much to attend to this word")
- *Values (V)*: Actual information of the words in the context
- *LayerNorm and Residual Connections*: ONLY placed after Attention Block and FFN Block (after we add the residuals (output of ATN/FFN) to the input, we want to "standardize" the scale of this output for training stability), and only between them, not within them.
    - Recall; there are LayerNorm weights for every parameter in the prior layer, these weights allow for "dynamic" scaling in the case where parameters have vastly different scales e.g. [1, 100, 20] (Think of "normalizing" the prior Layer's output)