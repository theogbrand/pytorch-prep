{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff4234e",
   "metadata": {},
   "source": [
    "One key thing to understand is how torch.matmul behaves based on the shapes of the input tensors.\n",
    "\n",
    "For example, if you have two 2D tensors representing matrices, say A with shape (m, n) and B with shape (n, p), it will compute the standard matrix product resulting in a tensor of shape (m, p). \n",
    "\n",
    "If the inputs are 1D tensors, which you can think of as vectors, it performs a dot product and returns a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d321dc",
   "metadata": {},
   "source": [
    "Things get more interesting with higher dimensions; The rules are similar with standard operations Tensor Broadcasting (**element wise ops**), but with a few twists.\n",
    "\n",
    "PyTorch treats **the last two dimensions** of each tensor as the \"core matrices\" to multiply, while **any preceding dimensions** are considered batch dimensions that can be **broadcasted together**\n",
    "\n",
    "The \"core matrics\" follow matrix multiplication rules.\n",
    "\n",
    "everything else (batch dims) follow standard broadcasting rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbfb93",
   "metadata": {},
   "source": [
    "Broadcasting happens primarily in the **batch dimensions**. \n",
    "\n",
    "The core matrix dimensions—**the last two of each tensor**—must match exactly for multiplication (**the inner dimensions need to be equal**), without broadcasting there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e2c1af",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The key difference:\n",
    "\n",
    "\t•In elementwise ops, all dimensions must broadcast to equal size.\n",
    "\t\n",
    "\t•In matmul, only batch dimensions broadcast; the matrix multiply dimensions must follow the linear algebra rule (…, n, m) @ (…, m, p).\n",
    "\n",
    "So if your shapes don’t match in the last two dims like (n, m) and (m, p), it’s not a broadcasting issue — it’s a linear algebra dimension mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd30964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Q: (batch, num_heads, seq_len, head_dim)\n",
    "# K: (batch, num_heads, seq_len, head_dim)\n",
    "# Attention scores: Q @ K^T\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "# Result: (batch, num_heads, seq_len, seq_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
